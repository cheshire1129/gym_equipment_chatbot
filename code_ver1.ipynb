{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cee4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# [최종 코드 제출 전 주석 처리 제거] 출력이 너무 길어 잠시 주석 처리하였음\n",
    "#for dirname, _, filenames in os.walk('.\\\\input_temp'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc74e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 디렉토리가 이미 존재하여 새로운 dataset 디렉토리를 만들기 전, 삭제하였습니다. \n",
      "새로운 dataset 디렉토리를 생성하였습니다.\n",
      "\n",
      "./input_temp\\Dumbells에서 ./dataset/training\\Dumbells로 파일 60개가 복사되었습니다.\n",
      "./input_temp\\Dumbells에서 ./dataset/validation\\Dumbells로 파일 20개가 복사되었습니다.\n",
      "./input_temp\\Dumbells에서 ./dataset/test\\Dumbells로 파일 20개가 복사되었습니다.\n",
      "\n",
      "./input_temp\\Elliptical Machine에서 ./dataset/training\\Elliptical Machine로 파일 60개가 복사되었습니다.\n",
      "./input_temp\\Elliptical Machine에서 ./dataset/validation\\Elliptical Machine로 파일 20개가 복사되었습니다.\n",
      "./input_temp\\Elliptical Machine에서 ./dataset/test\\Elliptical Machine로 파일 20개가 복사되었습니다.\n",
      "\n",
      "./input_temp\\Recumbent Bike에서 ./dataset/training\\Recumbent Bike로 파일 60개가 복사되었습니다.\n",
      "./input_temp\\Recumbent Bike에서 ./dataset/validation\\Recumbent Bike로 파일 20개가 복사되었습니다.\n",
      "./input_temp\\Recumbent Bike에서 ./dataset/test\\Recumbent Bike로 파일 20개가 복사되었습니다.\n",
      "\n",
      "category의 개수는 3개이고, \n",
      "category는 다음과 같습니다\n",
      "Dumbells \n",
      "Elliptical Machine \n",
      "Recumbent Bike \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 해당 셀은 full dataset(전체 데이터)을 data를 category별 6:2:2로 split한다.\n",
    "# split된 data는 각각 training/해당category, valivation/해당category, test/해당category 에 저장된다. \n",
    "\n",
    "# full dataset 디렉토리\n",
    "dataset_dir = './input_temp'\n",
    "\n",
    "if(os.path.isdir('./dataset')):\n",
    "    print(f\"dataset 디렉토리가 이미 존재하여 새로운 dataset 디렉토리를 만들기 전, 삭제하였습니다. \")\n",
    "    shutil.rmtree('./dataset')\n",
    "\n",
    "# train 디렉토리와 validation 디렉토리와 test 디렉토리 생성\n",
    "train_dir = './dataset/training'\n",
    "val_dir = './dataset/validation'\n",
    "test_dir = './dataset/test'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "print(\"새로운 dataset 디렉토리를 생성하였습니다.\")\n",
    "print()\n",
    "\n",
    "categories = ['Dumbells', 'Elliptical Machine', 'Recumbent Bike']\n",
    "category_num = len(categories)\n",
    "\n",
    "# Category별로 복사\n",
    "for category in categories :\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, category), exist_ok=True)\n",
    "\n",
    "    category_path = os.path.join(dataset_dir, category)\n",
    "\n",
    "    # Check if category path exists\n",
    "    if not os.path.exists(category_path):\n",
    "        print(f\"Path {category_path} does not exist \")\n",
    "\n",
    "    # Keep all files in category directory by list files_in_category\n",
    "    files_in_category = [file for file in os.listdir(category_path) if os.path.join(category_path, file)]\n",
    "\n",
    "    # split files by 6:2:2\n",
    "    train_files, val_files = train_test_split(files_in_category, test_size=0.4)\n",
    "    val_files, test_files = train_test_split(val_files, test_size=0.5)\n",
    "\n",
    "    \n",
    "    # Copy train files into ./dataset/training and ./dataset/validation and ./dataset/test\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(category_path, file), os.path.join(train_dir, category, file))\n",
    "    print(f\"{category_path}에서 {os.path.join(train_dir, category)}로 파일 {len(train_files)}개가 복사되었습니다.\")\n",
    "    \n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(category_path, file), os.path.join(val_dir, category, file))\n",
    "    print(f\"{category_path}에서 {os.path.join(val_dir, category)}로 파일 {len(val_files)}개가 복사되었습니다.\")\n",
    "    \n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(category_path, file), os.path.join(test_dir, category, file))\n",
    "    print(f\"{category_path}에서 {os.path.join(test_dir, category)}로 파일 {len(test_files)}개가 복사되었습니다.\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# category 확인\n",
    "print(f\"category의 개수는 {category_num}개이고, \")\n",
    "print(\"category는 다음과 같습니다\")\n",
    "for cat in categories:\n",
    "    print(f\"{cat} \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63322d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 files belonging to 3 classes.\n",
      "Found 60 files belonging to 3 classes.\n",
      "Image batch shape: (20, 150, 150, 3)\n",
      "Label batch shape: (20, 3)\n",
      "Labels: tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]], shape=(20, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Image 처리용\n",
    "#from tensorflow.keras.processing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Model 생성용\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Model layer용\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Optimizer용\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# train_dir, val_dir path \n",
    "train_dir = './dataset/training'\n",
    "val_dir = './dataset/validation'\n",
    "\n",
    "\n",
    "# Image 처리\n",
    "# train_dir/어떤category, val_dir/어떤category 안에 있는 이미지들을 해당 category로 labeling 해줌\n",
    "# model.fit에서 사용되는 batch_size로 배치 크기 설정 \n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    directory = train_dir,\n",
    "    image_size = (150, 150),\n",
    "    batch_size = 20,\n",
    "    label_mode='categorical',\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    directory = val_dir,\n",
    "    image_size = (150, 150),\n",
    "    label_mode='categorical',\n",
    "    batch_size = 10,\n",
    ")\n",
    "\n",
    "for images, labels in train_dataset.take(1):  # Take one batch\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26a55e99-43b8-47aa-bb97-59709068fdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3/9\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - accuracy: 0.4056 - loss: 10.5005"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nUnknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_26000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 34\u001b[0m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m              loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m              metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m              )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train \u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, \n\u001b[0;32m     35\u001b[0m          validation_data \u001b[38;5;241m=\u001b[39m val_dataset,\n\u001b[0;32m     36\u001b[0m          epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     37\u001b[0m          verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nUnknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_26000]"
     ]
    }
   ],
   "source": [
    "# Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.Rescaling(1.0 / 255),       # RGB normalize\n",
    "    layers.RandomFlip(\"horizontal\"),   # 좌우 반전\n",
    "    layers.RandomRotation(0.2),        # 회전\n",
    "    layers.RandomZoom(0.2),            # 확대, 축소\n",
    "    layers.RandomTranslation(0.1, 0.1) # 좌우 이동\n",
    "])\n",
    "\n",
    "# Define Model \n",
    "\n",
    "# [최종 코드 제출 전 검토]모델 생성 방법 주석 처리\n",
    "# CNN으로 3 X 3 filter 32개 사용 \n",
    "# MaxPooling\n",
    "# FC Layer (hidden units : 128)\n",
    "# FC Layer (hidden units : category 개수)\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(category_num, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# Train \n",
    "history = model.fit(train_dataset, \n",
    "         validation_data = val_dataset,\n",
    "         epochs=50,\n",
    "         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c59727-6eee-453b-8a9c-dc4b5c348648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdbb51-8eb7-4a9e-88ae-61b72f4abfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [최종 코드 제출 전 해당 셀 삭제]\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d53a2-ec50-475a-8a79-5bcb934cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0124-e03e-4eac-9618-2c93dcb1771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 정확도가 높은 모델로 test data predict하고 Visualize (confusion matrix)\n",
    "# test data image visualize와 함께 predict label과 actual label 표기\n",
    "\n",
    "# Test data predication 을 통해 최종 accuracy 확인\n",
    "\n",
    "# test_dir path \n",
    "test_dir = './dataset/test'\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    directory = test_dir,\n",
    "    image_size = (150, 150),\n",
    "    label_mode='categorical',\n",
    "    batch_size=80\n",
    ")\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for _, labels in test_dataset:\n",
    "    all_labels.append(np.argmax(labels.numpy(), axis=1))\n",
    "all_labels = np.concatenate(all_labels, axis=0)  \n",
    "\n",
    "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
    "\n",
    "#for label in y_pred:\n",
    "#    print(categories[label])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(all_labels, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b335ef-60ed-4969-a50c-cc974513ee46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5f05f-cc1c-414d-a7ff-469efed3338b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
